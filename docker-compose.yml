# SemInstruct - Local Development
# Lightweight proxy to shimmy for OpenAI-compatible inference
# seminstruct proxies requests to shimmy which handles actual model inference

services:
  # Lightweight proxy service (~256MB memory)
  seminstruct:
    build:
      context: .
      dockerfile: Dockerfile
    image: semstreams-seminstruct:dev
    container_name: seminstruct-dev
    ports:
      - "8083:8083"
    environment:
      - SEMINSTRUCT_SHIMMY_URL=http://shimmy:11435
      - SEMINSTRUCT_PORT=8083
      - SEMINSTRUCT_TIMEOUT_SECONDS=120
      - SEMINSTRUCT_MAX_RETRIES=3
      - RUST_LOG=info
    depends_on:
      shimmy:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '1.0'
        reservations:
          memory: 64M
          cpus: '0.25'

  # Shimmy inference backend with baked-in GGUF model
  # Model is downloaded at build time, not runtime
  shimmy:
    build:
      context: .
      dockerfile: Dockerfile.shimmy
      args:
        # Model tiers:
        # CI:     TinyLlama-1.1B Q4_K_M (~660MB) - default, fits GitHub Actions
        # Dev:    Mistral-7B Q4_K_M (~4.1GB) - good local balance
        # Prod:   Mistral-7B Q6_K (~5.5GB) - production quality
        #
        # Override: MODEL_FILE=mistral-7b-instruct-v0.2.Q4_K_M.gguf docker compose build
        #           MODEL_REPO=TheBloke/Mistral-7B-Instruct-v0.2-GGUF
        MODEL_REPO: ${MODEL_REPO:-TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF}
        MODEL_FILE: ${MODEL_FILE:-tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf}
    image: shimmy:local
    container_name: shimmy-dev
    ports:
      - "11435:11435"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11435/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Model baked in, just needs to load
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 10G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
