# SemInstruct - Local Development
# Lightweight proxy to shimmy for OpenAI-compatible inference
# seminstruct proxies requests to shimmy which handles actual model inference

services:
  # Lightweight proxy service (~256MB memory)
  seminstruct:
    build:
      context: .
      dockerfile: Dockerfile
    image: semstreams-seminstruct:dev
    container_name: seminstruct-dev
    ports:
      - "8083:8083"
    environment:
      # Shimmy backend configuration
      - SEMINSTRUCT_SHIMMY_URL=http://shimmy:8080
      - SEMINSTRUCT_PORT=8083
      - SEMINSTRUCT_TIMEOUT_SECONDS=120
      - SEMINSTRUCT_MAX_RETRIES=3
      # Logging
      - RUST_LOG=info
    depends_on:
      shimmy:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s  # Fast startup - no model loading
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '1.0'
        reservations:
          memory: 64M
          cpus: '0.25'

  # Shimmy inference backend (heavy lifting)
  shimmy:
    image: ghcr.io/michael-a-kuykendall/shimmy:latest
    container_name: shimmy-dev
    ports:
      - "8080:8080"
    environment:
      # Model configuration
      - SHIMMY_MODEL=mistral-7b-instruct
      - SHIMMY_PORT=8080
    volumes:
      # Persist model cache to avoid re-downloading
      - shimmy-cache:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s  # Allow time for model download
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 10G
          cpus: '4.0'
        reservations:
          memory: 6G
          cpus: '2.0'

volumes:
  shimmy-cache:
    driver: local
