# SemInstruct - Local Development
# Lightweight proxy to shimmy for OpenAI-compatible inference
# seminstruct proxies requests to shimmy which handles actual model inference

services:
  # Lightweight proxy service (~256MB memory)
  seminstruct:
    build:
      context: .
      dockerfile: Dockerfile
    image: semstreams-seminstruct:dev
    container_name: seminstruct-dev
    ports:
      - "8083:8083"
    environment:
      - SEMINSTRUCT_SHIMMY_URL=http://shimmy:11435
      - SEMINSTRUCT_PORT=8083
      - SEMINSTRUCT_TIMEOUT_SECONDS=120
      - SEMINSTRUCT_MAX_RETRIES=3
      - RUST_LOG=info
    depends_on:
      shimmy:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '1.0'
        reservations:
          memory: 64M
          cpus: '0.25'

  # Shimmy inference backend with Qwen2.5-0.5B baked in
  # Pre-built image from GHCR - no build required
  # For custom models, use Dockerfile.shimmy instead
  shimmy:
    image: ghcr.io/c360studio/semshimmy:latest
    container_name: shimmy-dev
    ports:
      - "11435:11435"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11435/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s  # Model already loaded in image
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '4.0'
        reservations:
          memory: 1G
          cpus: '1.0'
