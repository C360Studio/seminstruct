[package]
name = "seminstruct"
version = "0.2.0"
edition = "2021"
authors = ["C360 Team"]
description = "Lightweight proxy service for OpenAI-compatible LLM inference via shimmy"

[dependencies]
# HTTP server
axum = "0.7"
tokio = { version = "1", features = ["full"] }
tower = "0.4"
tower-http = { version = "0.5", features = ["trace", "cors"] }

# HTTP client for shimmy proxy
reqwest = { version = "0.12", features = ["json"] }

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Utilities
uuid = { version = "1", features = ["v4"] }

# Error handling
anyhow = "1"
thiserror = "1"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# Metrics
prometheus = "0.13"

[dev-dependencies]
# Testing
mockito = "1.6"

[profile.release]
lto = true
codegen-units = 1
opt-level = 3
strip = true

[[bin]]
name = "seminstruct"
path = "src/main.rs"
