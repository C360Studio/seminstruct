# Shimmy build with GGUF model baked in
# Downloads model at build time for instant startup

FROM rust:1.85-slim AS builder

RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    build-essential \
    git \
    clang \
    libclang-dev \
    cmake \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Clone shimmy from source
RUN git clone --depth 1 https://github.com/michael-a-kuykendall/shimmy.git .

# Build the application (skip benches that cause build issues)
RUN cargo build --release --features huggingface --lib --bins

# Model download stage - downloads GGUF at build time
FROM python:3.11-slim AS model-downloader

RUN pip install --no-cache-dir huggingface-hub

WORKDIR /models

# Build args for model selection
# CI default: TinyLlama-1.1B Q4_K_M (~660MB) - small enough for GitHub Actions
# Production: Override with Mistral-7B variants for better quality
ARG MODEL_REPO=TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
ARG MODEL_FILE=tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

RUN python -c "from huggingface_hub import hf_hub_download; hf_hub_download('${MODEL_REPO}', '${MODEL_FILE}', local_dir='.', local_dir_use_symlinks=False)"

# Runtime stage
FROM debian:bookworm-slim

RUN apt-get update && apt-get install -y \
    ca-certificates \
    libssl3 \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY --from=builder /app/target/release/shimmy /usr/local/bin/shimmy

# Copy downloaded GGUF model
COPY --from=model-downloader /models/*.gguf /app/models/

EXPOSE 11435

# Bind to 0.0.0.0:11435 for container networking
CMD ["shimmy", "serve", "--bind", "0.0.0.0:11435"]
